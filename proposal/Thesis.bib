

@article{IDS,
	author = {Reinsel, David and John Gantz and John Rydning},
	journal = {International Data Corporation},
	title = {The Digitization of the World: From Edge to Core},
	month = {November},
	year = {2018},
	pages = {3}
}

@article{genome_size,
    author = {Christley, Scott and Lu, Yiming and Li, Chen and Xie, Xiaohui},
    title = "{Human genomes as email attachments}",
    journal = {Bioinformatics},
    volume = {25},
    number = {2},
    pages = {274-275},
    year = {2008},
    month = {09},
    abstract = {genome sequences can fit as an email attachment. i use this to refer to the scale of big data},
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btn582},
    url = {https://doi.org/10.1093/bioinformatics/btn582},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/25/2/274/16888832/btn582.pdf},
}


@article{virtual_memory_tlb,
author = {Basu, Arkaprava and Gandhi, Jayneel and Chang, Jichuan and Hill, Mark D. and Swift, Michael M.},
title = {Efficient Virtual Memory for Big Memory Servers},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/2508148.2485943},
doi = {10.1145/2508148.2485943},
abstract = {TODO},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {237–248},
numpages = {12},
keywords = {virtual memory, tanslation lookaside buffer}
}

@inproceedings{STAPL,
author = {Rauchwerger, Lawrence and Arzu, Francisco and Ouchi, Koji},
title = {Standard Templates Adaptive Parallel Library (STAPL)},
year = {1998},
isbn = {3540651721},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {TODO},
booktitle = {Selected Papers from the 4th International Workshop on Languages, Compilers, and Run-Time Systems for Scalable Computers},
pages = {402–409},
numpages = {8},
series = {LCR '98}
}
}

@inproceedings{STAPL_2010,
author = {Buss, Antal and Harshvardhan and Papadopoulos, Ioannis and Pearce, Olga and Smith, Timmie and Tanase, Gabriel and Thomas, Nathan and Xu, Xiabing and Bianco, Mauro and Amato, Nancy M. and Rauchwerger, Lawrence},
title = {STAPL: Standard Template Adaptive Parallel Library},
year = {2010},
isbn = {9781605589084},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.miamioh.edu/10.1145/1815695.1815713},
doi = {10.1145/1815695.1815713},
abstract = {TODO},
booktitle = {Proceedings of the 3rd Annual Haifa Experimental Systems Conference},
articleno = {14},
numpages = {10},
keywords = {parallel data structures, high productivity parallel programming, library},
location = {Haifa, Israel},
series = {SYSTOR '10}
}

@inproceedings{practical_dist_c,
author = {Drocco, Maurizio and Castellana, Vito Giovanni and Minutoli, Marco},
title = {Practical Distributed Programming in C++},
year = {2020},
isbn = {9781450370523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3369583.3392680},
doi = {10.1145/3369583.3392680},
abstract = {TODO},
pages = {35–39},
numpages = {5},
keywords = {distributed data structures, C++, distributed programming},
location = {Stockholm, Sweden},
series = {HPDC '20}
}

@inproceedings{taskflow,
author = {Lin, Chun-Xun and Huang, Tsung-Wei and Guo, Guannan and Wong, Martin D. F.},
title = {A Modern C++ Parallel Task Programming Library},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3350537},
doi = {10.1145/3343031.3350537},
abstract = {TODO},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {2284–2287},
numpages = {4},
keywords = {task dependency graph, parallel programming, task parallelism},
location = {Nice, France},
series = {MM '19}
}

@inproceedings{towards_dist_cpp,
author = {Brown, Gordon and Reyes, Ruyman and Wong, Michael},
title = {Towards Heterogeneous and Distributed Computing in C++},
year = {2019},
isbn = {9781450362306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318170.3318196},
doi = {10.1145/3318170.3318196},
abstract = {TODO},
booktitle = {Proceedings of the International Workshop on OpenCL},
articleno = {18},
numpages = {5},
keywords = {heterogeneous programming, executors, C++, parallelism, distributed programming models, concurrency},
location = {Boston, MA, USA},
series = {IWOCL'19}
}

@inproceedings{10.1145/1941553.1941586,
author = {Tanase, Gabriel and Buss, Antal and Fidel, Adam and Harshvardhan and Papadopoulos, Ioannis and Pearce, Olga and Smith, Timmie and Thomas, Nathan and Xu, Xiabing and Mourad, Nedal and Vu, Jeremy and Bianco, Mauro and Amato, Nancy M. and Rauchwerger, Lawrence},
title = {The STAPL Parallel Container Framework},
year = {2011},
isbn = {9781450301190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.miamioh.edu/10.1145/1941553.1941586},
doi = {10.1145/1941553.1941586},
abstract = {TODO},
booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
pages = {235–246},
numpages = {12},
keywords = {structures, containers, languages, data, parallel, libraries},
location = {San Antonio, TX, USA},
series = {PPoPP '11}
}

  

@article{stapl_parallel_container,
author = {Tanase, Gabriel and Buss, Antal and Fidel, Adam and Harshvardhan and Papadopoulos, Ioannis and Pearce, Olga and Smith, Timmie and Thomas, Nathan and Xu, Xiabing and Mourad, Nedal and Vu, Jeremy and Bianco, Mauro and Amato, Nancy M. and Rauchwerger, Lawrence},
title = {The STAPL Parallel Container Framework},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {8},
issn = {0362-1340},
url = {https://doi-org.proxy.lib.miamioh.edu/10.1145/2038037.1941586},
doi = {10.1145/2038037.1941586},
abstract = {TODO},
journal = {SIGPLAN Not.},
month = feb,
pages = {235–246},
numpages = {12},
keywords = {libraries, containers, data, structures, parallel, languages}
}

@book{stl_guide, 
place={Boston, MA}, 
title={STL tutorial and reference guide: C++ programming with the standard template library}, 
publisher={Addison-Wesley}, 
author={Musser, David R. and Derge, Gillmer J. and Saini, Atul}, year={2005}
} 

@book{the_cpp_programming_language, 
place={Upper Saddle River i pozostałe}, 
title={The C++ programming language}, 
publisher={Pearson}, 
author={Stroustrup, Bjarne}, 
year={2018}} 






@INPROCEEDINGS{sharing_cpu_memory,

  author={ {Xiaodong Zhang} and  {Yanxia Qu} and  {Li Xiao}},

  booktitle={Proceedings 20th IEEE International Conference on Distributed Computing Systems}, 

  title={Improving distributed workload performance by sharing both CPU and memory resources}, 

  year={2000},

  volume={},

  number={},
  abstract={todo},

  pages={233-241},

  doi={10.1109/ICDCS.2000.840934}}

 @book{intel_tbb, 
 place={Hillsboro, OR}, 
 title={Multi-core programming: increasing performance through software multi-threading}, 
 publisher={Intel Press}, 
 abstract={TODO},
 author={Akhter, Shameem and Roberts, Jason}, 
 year={2006}
 } 
 
 
 
 @inproceedings{parallel_programming_w_charm,
author = {Acun, Bilge and Gupta, Abhishek and Jain, Nikhil and Langer, Akhil and Menon, Harshitha and Mikida, Eric and Ni, Xiang and Robson, Michael and Sun, Yanhua and Totoni, Ehsan and Wesolowski, Lukasz and Kale, Laxmikant},
title = {Parallel Programming with Migratable Objects: Charm++ in Practice},
year = {2014},
isbn = {9781479955008},
publisher = {IEEE Press},
url = {https://doi-org.proxy.lib.miamioh.edu/10.1109/SC.2014.58},
doi = {10.1109/SC.2014.58},
abstract = {TODO},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
pages = {647–658},
numpages = {12},
location = {New Orleans, Louisana},
series = {SC '14}
}

@article{ilp_history, 
 title={Instruction-Level Parallel Processing: History, Overview and Perspective},
 volume={7}, 
 DOI={10.1023/b:supe.0000022106.06579.c4}, 
 number={1}, 
 journal={The Journal of Supercomputing}, 
 author={Rau, B Ramakrishna and Fisher, Joseph A.}, 
 year={1993},
 abstract={summary of the history of instruction-level parallelism from its inception to 1993 with specific attention to supercomputing. i use this mainly for its definition of instruction-level parallelism to contract with thread-based parallelism }
 } 
 
 @inbook{hpc_openstax, 
 title={High Performance Computing Chapter 5}, 
 booktitle={High Performance Computing}, 
 publisher={OpenStax-CNX}, 
 author={Dowd, Kevin and Severance, Charles}, 
 year={2010},
 abstract={Good open textbook on high-performance computing. this chapter on parallelism used for information on different kinds of parallelism. note: can probably use different chapters for different things}
 } 
 

@INPROCEEDINGS{chapel,
  author={D. {Callahan} and B. L. {Chamberlain} and H. P. {Zima}},
  booktitle={Ninth International Workshop on High-Level Parallel Programming Models and Supportive Environments, 2004. Proceedings.}, 
  abstract={TODO},
  title={The cascade high productivity language}, 
  year={2004},
  volume={},
  number={},
  pages={52-60},
  doi={10.1109/HIPS.2004.1299190}
}


@article{X10,
author = {Charles, Philippe and Grothoff, Christian and Saraswat, Vijay and Donawa, Christopher and Kielstra, Allan and Ebcioglu, Kemal and von Praun, Christoph and Sarkar, Vivek},
title = {X10: An Object-Oriented Approach to Non-Uniform Cluster Computing},
year = {2005},
issue_date = {October 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/1103845.1094852},
doi = {10.1145/1103845.1094852},
abstract = {TODO},
journal = {SIGPLAN Not.},
month = oct,
pages = {519–538},
numpages = {20},
keywords = {atomic blocks, data distribution, productivity, partitioned global address space (PGAS), X10, Java, non-uniform cluster computing (NUCC), places, scalability, clocks, multithreading}
}




@misc{memcached, 
 title={a distributed memory object caching system}, url={https://www.memcached.org/}, 
 journal={memcached}, 
 author={Fitzpatrick, B.}
} 
 
 @INPROCEEDINGS{spark,
  author={J. {Fu} and J. {Sun} and K. {Wang}},
  booktitle={2016 International Conference on Industrial Informatics - Computing Technology, Intelligent Technology, Industrial Information Integration (ICIICII)}, 
  title={SPARK – A Big Data Processing Platform for Machine Learning}, 
  abstract={TODO},
  year={2016},
  volume={},
  number={},
  pages={48-51},
  doi={10.1109/ICIICII.2016.0023}}
  
 @article{zookeeper,
author = {Hunt, Patrick and Konar, Mahadev and Grid, Yahoo and Junqueira, Flavio and Reed, Benjamin and Research, Yahoo},
year = {2010},
month = {06},
pages = {},
abstract={TODO},
title = {ZooKeeper: Wait-free Coordination for Internet-scale Systems},
volume = {8},
journal = {ATC. USENIX}
}

@article{GAM,
author = {Cai, Qingchao and Guo, Wentian and Zhang, Hao and Agrawal, Divyakant and Chen, Gang and Ooi, Beng Chin and Tan, Kian-Lee and Teo, Yong Meng and Wang, Sheng},
title = {Efficient Distributed Memory Management with RDMA and Caching},
year = {2018},
issue_date = {July 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3236187.3236209},
doi = {10.14778/3236187.3236209},
abstract = {TODO},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {1604–1617},
numpages = {14}
}

@inproceedings{stapl_rts,
author = {Papadopoulos, Ioannis and Thomas, Nathan and Fidel, Adam and Amato, Nancy M. and Rauchwerger, Lawrence},
title = {STAPL-RTS: An Application Driven Runtime System},
year = {2015},
isbn = {9781450335591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2751205.2751233},
doi = {10.1145/2751205.2751233},
abstract={TODO},
booktitle = {Proceedings of the 29th ACM on International Conference on Supercomputing},
pages = {425–434},
numpages = {10},
keywords = {shared memory, application driven optimizations, distributed memory, data flow, parallel programming, remote method invocation, runtime systems},
location = {Newport Beach, California, USA},
series = {ICS '15}
}

@inbook{dist_systems_concepts, 
	place={Boston, MA}, 
	title={Distributed systems: Concepts and design, Chapter 1}, 
	booktitle={Distributed systems: concepts and design}, 
	publisher={Addison-Wesley}, 
	author={Coulouris, George F.}, 
	year={2011},
	abstract={Textbook about all kinds of distributed systems. I mainly use the beginning for its characterization of distributed systems and the challenges therein (parallel vs distributed computing section)}
} 





@misc{n1875,
	title={N1875: C++ Threads},
	author={Lawrence Crowl},
	howpublished={\url{wg21.link/n1875}}
}

@misc{n1682,
	title={N1682: A Multi-threading Library for Standard C++},
	author={Pete Becker},
	howpublished={\url{wg21.link/n1682}}
}
@misc{n1815,
	title={N1815: ISO C++ Strategic Plan for Multithreading},
	author={Lawrence Crowl},
	howpublished={\url{wg21.link/n1815}}
}
@misc{n1883,
	title={N1883: Preliminary Threading Library Proposal for TR2},
	author={Kevlin Henney},
	howpublished={\url{wg21.link/n1883}}
}
@misc{n1907,
	title={N1907: A Multi-Threading Library for Standard C++, Revision 1},
	author={Pete Becker},
	howpublished={\url{wg21.link/n1907}}
}
@misc{n2043,
	title={N2043: Simplifying And Extending Mutex and Scoped Lock Types For C++ Multi-Threading Library},
	author={Ion Gaztañaga},
	howpublished={\url{wg21.link/n2043}}
}

@misc{n2094,
	title={N2094: Multithreading API for C++0X - A Layered Approach},
	author={Howard E. Hinnant},
	howpublished={\url{wg21.link/n2094}}
}
	
@misc{n2096,
	title={N2096: Transporting Values and Exceptions Between Threads },
	author={Peter Dimov},
	howpublished={\url{wg21.link/n2096}}
}
@misc{n2139,
	title={N2139: Thoughts on a Thread Library for C++ },
	author={Anthony Williams},
	howpublished={\url{wg21.link/n2139}}
}
@misc{n2178,
	title={N2178: Proposed Text for Chapter 30, Thread Support Library [threads] },
	author={Thoughts on a Thread Library for C++},
	howpublished={\url{wg21.link/n2139}}
}

@misc{n2184,
	title={N2184: Thread Launching for C++},
	author={Howard E. Hinnant},
	howpublished={\url{wg21.link/n2184}}
}

@misc{n2285,
	title={N2285: A Multi-threading Library for Standard C++, Revision 2},
	author={Pete Becker},
	howpublished={\url{wg21.link/n2285}}
}
@misc{n2889,
	title={N2889: An Asynchronous Call for C++},
	author={Lawrence Crowl},
	howpublished={\url{wg21.link/n2889}}
}

@misc{n2320,
	title={N2320: Multi-threading Library for Standard C++},
	author={Howard E. Hinnant and Beman Dawes and Lawrence Crowl and Jeff Gardland and Anthony Williams},
	howpublished={\url{wg21.link/n2320}}
}

@misc{p0443,
	title={P0443: Multi-threading Library for Standard C++},
	author={Hoberock, et. al.},
	howpublished={\url{wg21.link/p0443}}
}


@inproceedings{stapl_skeleton_framework,
author = {Zandifar, Mani and Abdul Jabbar, Mustafa and Majidi, Alireza and Keyes, David and Amato, Nancy M. and Rauchwerger, Lawrence},
title = {Composing Algorithmic Skeletons to Express High-Performance Scientific Applications},
year = {2015},
isbn = {9781450335591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2751205.2751241},
doi = {10.1145/2751205.2751241},
abstract = {Algorithmic skeletons are high-level representations for parallel programs that hide the underlying parallelism details from program specification. These skeletons are defined in terms of higher-order functions that can be composed to build larger programs. Many skeleton frameworks support efficient implementations for stand-alone skeletons such as map, reduce, and zip for both shared-memory systems and small clusters. However, in these frameworks, expressing complex skeletons that are constructed through composition of fundamental skeletons either requires complete reimplementation or suffers from limited scalability due to required global synchronization. In the STAPL Skeleton Framework, we represent skeletons as parametric data flow graphs and describe composition of skeletons by point-to-point dependencies of their data flow graph representations. As a result, we eliminate the need for reimplementation and global synchronizations in composed skeletons. In this work, we describe the process of translating skeleton-based programs to data flow graphs and define rules for skeleton composition. To show the expressivity and ease of use of our framework, we show skeleton-based representations of the NAS EP, IS, and FT benchmarks. To show reusability and applicability of our framework on real-world applications we show an N-Body application using the FMM (Fast Multipole Method) hierarchical algorithm. Our results show that expressivity can be achieved without loss of performance even in complex real-world applications.},
booktitle = {Proceedings of the 29th ACM on International Conference on Supercomputing},
pages = {415–424},
numpages = {10},
keywords = {patterns, algorithmic skeletons, high-performance computing, data flow programming, distributed systems},
location = {Newport Beach, California, USA},
series = {ICS '15}
}


@inproceedings{stapl_parray,
author = {Tanase, Gabriel and Bianco, Mauro and Amato, Nancy M. and Rauchwerger, Lawrence},
title = {The STAPL PArray},
year = {2007},
isbn = {9781595938077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.miamioh.edu/10.1145/1327171.1327180},
doi = {10.1145/1327171.1327180},
abstract = {The Standard Template Adaptive Parallel Library (STAPL) is a parallel programming framework that extends C++ and STL with support for parallelism. STAPL provides parallel data structures (pContainers) and generic parallel algorithms (pAlgorithms), and a methodology for extending them to provide customized functionality. STAPL pContainers are thread-safe, concurrent objects, i.e., shared objects that provide parallel methods that can be invoked concurrently. They provide views as a generic means to access data that can be passed as input to generic pAlgorithms.In this work, we present the STAPL pArray, the parallel equivalent of the sequential STL valarray, a fixed-size data structure optimized for storing and accessing data based on one-dimensional indices. We describe the pArray design and show how it can support a variety of underlying data distribution policies currently available in STAPL, such as blocked or blocked cyclic. We provide experimental results showing that pAlgorithms using the pArray scale well to more than 2,000 processors. We also provide results using different data distributions that illustrate that the performance of pAlgorithms and pArray methods is usually sensitive to the underlying data distribution, and moreover, that there is no one data distribution that performs best for all pAlgorithms, processor counts, or machines.},
booktitle = {Proceedings of the 2007 Workshop on MEmory Performance: DEaling with Applications, Systems and Architecture},
pages = {73–80},
numpages = {8},
location = {Brasov, Romania},
series = {MEDEA '07}
}

@inproceedings{stapl_graph,
author = {Harshvardhan and Amato, Nancy M. and Rauchweger, Lawrence},
title = {Processing Big Data Graphs on Memory-Restricted Systems},
year = {2014},
isbn = {9781450328098},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.miamioh.edu/10.1145/2628071.2671429},
doi = {10.1145/2628071.2671429},
abstract = {With the advent of big-data, processing large graphs quickly has become increasingly important. Most existing approaches either utilize in-memory processing techniques, which can only process graphs that fit completely in RAM, or disk-based techniques that sacrifice performance.In this work, we propose a novel RAM-Disk hybrid approach to graph processing that can scale well from a single shared-memory node to large distributed-memory systems. It works by partitioning the graph into subgraphs that fit in RAM and uses a paging-like technique to load subgraphs. We show that without modifying the algorithms, this approach can scale from small memory-constrained systems (such as tablets) to large-scale distributed machines with 16,000+ cores.},
booktitle = {Proceedings of the 23rd International Conference on Parallel Architectures and Compilation},
pages = {517–518},
numpages = {2},
keywords = {out-of-core graph algorithms, parallel graph processing, distributed computing, graph analytics, big data},
location = {Edmonton, AB, Canada},
series = {PACT '14}
}

@misc{euler_1,
	title={Project Euler: Problem 1},
	howpublished={\url{projecteuler.net/problem=1}}
}

@misc{stapl_gitlab,
	title={Gitlab.com: Standard Adaptive Parallel Templating Library},
	howpublished={\url{gitlab.com/parasol-lab/stapl}}
}

@inproceedings{10.1145/165854.165874,
author = {Kale, Laxmikant V. and Krishnan, Sanjeev},
title = {CHARM++: A Portable Concurrent Object Oriented System Based on C++},
year = {1993},
isbn = {0897915879},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/165854.165874},
doi = {10.1145/165854.165874},
booktitle = {Proceedings of the Eighth Annual Conference on Object-Oriented Programming Systems, Languages, and Applications},
pages = {91–108},
numpages = {18},
location = {Washington, D.C., USA},
series = {OOPSLA '93}
}

  

@article{charm_93,
author = {Kale, Laxmikant V. and Krishnan, Sanjeev},
title = {CHARM++: A Portable Concurrent Object Oriented System Based on C++},
year = {1993},
issue_date = {Oct. 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/167962.165874},
doi = {10.1145/167962.165874},
journal = {SIGPLAN Not.},
month = oct,
pages = {91–108},
numpages = {18}
}

  
@article{charm_rts,
    author  = "L. V. Kal{\'e} and B. Ramkumar and A. B. Sinha and V. A. Saletore",
    title   = "{The CHARM Parallel Programming Language and System:
            Part II -- The Runtime system}",
    journal = "Parallel Programming Laboratory Technical Report \#95-03",
    year    = 1994
}


@misc{charm_tutorial,
	title={Charm Tutorial},
	howpublished={\url{charmplusplus.org/CharmConcepts.html}}
}

@inproceedings{X10_2005, title={X10}, url={http://dx.doi.org/10.1145/1094811.1094852}, DOI={10.1145/1094811.1094852}, booktitle={Proceedings of the 20th annual ACM SIGPLAN conference on Object oriented programming systems languages and applications - OOPSLA ’05}, publisher={ACM Press}, author={Charles, Philippe and Grothoff, Christian and Saraswat, Vijay and Donawa, Christopher and Kielstra, Allan and Ebcioglu, Kemal and von Praun, Christoph and Sarkar, Vivek}, year={2005} }

@book{dist_java, 
 title={Distributed computing in Java 9: make the best of Java for distributing applications}, publisher={Packt}, author={Rao, Pattamsetti Raja Malleswara}, year={2017}} 


@misc{X10_site,
	title={The X10 Programming Language},
	howpublished={\url{x10-lang.org}}
}

@article{chapel_2007,
author = {B.L. Chamberlain and D. Callahan and H.P. Zima},
title ={Parallel Programmability and the Chapel Language},
journal = {The International Journal of High Performance Computing Applications},
volume = {21},
number = {3},
pages = {291-312},
year = {2007},
doi = {10.1177/1094342007078442},

URL = { 
        https://doi.org/10.1177/1094342007078442
    
},
eprint = { 
        https://doi.org/10.1177/1094342007078442
    
}
,
    abstract = { In this paper we consider productivity challenges for parallel programmers and explore ways that parallel language design might help improve end-user productivity. We offer a candidate list of desirable qualities for a parallel programming language, and describe how these qualities are addressed in the design of the Chapel language. In doing so, we provide an overview of Chapel's features and how they help address parallel productivity. We also survey current techniques for parallel programming and describe ways in which we consider them to fall short of our idealized productive programming model. }
}



@misc{chapel_rts_ppt,
	title={The Chapel Runtime},
	author={Greg Titus},
	howpublished={\url{chapel-lang.org/presentations/Chapel-Runtime-Charm++13.pdf}}
}

@misc{chapel_github,
	title={Github: Chapel Language},
	howpublished={\url{https://github.com/chapel-lang/chapel}}
}



@misc{memcached_linux,
	title={Distributed Caching with Memcached},
	author={Brad Fitzpatrick},
	howpublished={\url{linuxjournal.com/article/7451}}
}

@misc{lj_dev,
	title={memcached: lj\_dev - LiveJournal},
	author={Brad Fitzpatrick},
	howpublished={\url{lj-dev.livejournal.com/539656.html}}
}


@INPROCEEDINGS{challenge_dist_mem,

  author={ {Nian-Feng Tzeng} and S. J. {Wallach}},

  booktitle={Proceedings International Conference on Computer Design. VLSI in Computers and Processors}, 

  title={Issues on the architecture and the design of distributed shared memory systems}, 

  year={1996},

  volume={},

  number={},

  pages={60-61},

  doi={10.1109/ICCD.1996.563532}}
  

@misc{vt_cs,
	title={Distributed Shared Memory},
	howpublished={\url{courses.cs.vt.edu/~cs5204/fall99/distributedSys/amento/dsm.html}}
}
@misc{amd_x86_64,
	title={System V Application Binary Interface},
	howpublished={\url{uclibc.org/docs/psABI-x86_64.pdf}}
}

@incollection{virtualization,
title = {Chapter 2 - Virtualization},
editor = {Dijiang Huang and Huijun Wu},
booktitle = {Mobile Cloud Computing},
publisher = {Morgan Kaufmann},
pages = {31-64},
year = {2018},
isbn = {978-0-12-809641-3},
doi = {https://doi.org/10.1016/B978-0-12-809641-3.00003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012809641300003X},
author = {Dijiang Huang and Huijun Wu},
keywords = {Virtualization, Hypervisor, Abstraction, Containers, Virtual network, Software defined network},
abstract = {Virtualization is the act of creating a virtual version of computing hardware, storage devices, and computer network resources. There are three classifications of virtualization technologies. The first classification is based on the internal process model. The second classification covers most of virtualization techniques at different levels of implementation within a computer. The third classification introduces two types of hypervisor. Besides virtualization through hypervisors, the container is a process-based lightweight virtualization, which has gained its momentum recently due to its efficiency and mobility. Besides computer virtualization, network virtualization (e.g., virtual networks, SDN, NFV, etc.), mobile virtualization (e.g., KVM over ARM), and storage virtualization (e.g., blob store, file store, etc.) are also important virtualization technologies. In this chapter, various virtualization techniques are surveyed, which form the foundation to build cloud computing service models.}
}

@INPROCEEDINGS{mpi,

author={},

booktitle={Supercomputing '93:Proceedings of the 1993 ACM/IEEE Conference on Supercomputing},

title={MPI: A message passing interface},

year={1993},

volume={},

number={},

pages={878-883},

doi={10.1109/SUPERC.1993.1263546}}

@InProceedings{ARMCI,
author="Nieplocha, Jarek
and Carpenter, Bryan",
editor="Rolim, Jos{\'e}
and Mueller, Frank
and Zomaya, Albert Y.
and Ercal, Fikret
and Olariu, Stephan
and Ravindran, Binoy
and Gustafsson, Jan
and Takada, Hiroaki
and Olsson, Ron
and Kale, Laxmikant V.
and Beckman, Pete
and Haines, Matthew
and ElGindy, Hossam
and Caromel, Denis
and Chaumette, Serge
and Fox, Geoffrey
and Pan, Yi
and Li, Keqin
and Yang, Tao
and Chiola, G.
and Conte, G.
and Mancini, L. V.
and M{\'e}ry, Domenique
and Sanders, Beverly
and Bhatt, Devesh
and Prasanna, Viktor",
title="ARMCI: A portable remote memory copy library for distributed array libraries and compiler run-time systems",
booktitle="Parallel and Distributed Processing",
year="1999",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="533--546",
abstract="This paper introduces a new portable communication library called ARMCI. ARMCI provides one-sided communication capabilities for distributed array libraries and compiler run-time systems. It supports remote memory copy, accumulate, and synchronization operations optimized for non-contiguous data transfers including strided and generalized UNIX I/O vector interfaces. The library has been employed in the Global Arrays shared memory programming toolkit and Adlib, a Parallel Compiler Run-time Consortium run-time system.",
isbn="978-3-540-48932-0"
}

@article{metagenomics,
    doi = {10.1371/journal.pone.0239741},
    author = {Abuín, José M. AND Lopes, Nuno AND Ferreira, Luís AND Pena, Tomás F. AND Schmidt, Bertil},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Big Data in metagenomics: Apache Spark vs MPI},
    year = {2020},
    month = {10},
    volume = {15},
    url = {https://doi.org/10.1371/journal.pone.0239741},
    pages = {1-20},
    abstract = {The progress of next-generation sequencing has lead to the availability of massive data sets used by a wide range of applications in biology and medicine. This has sparked significant interest in using modern Big Data technologies to process this large amount of information in distributed memory clusters of commodity hardware. Several approaches based on solutions such as Apache Hadoop or Apache Spark, have been proposed. These solutions allow developers to focus on the problem while the need to deal with low level details, such as data distribution schemes or communication patterns among processing nodes, can be ignored. However, performance and scalability are also of high importance when dealing with increasing problems sizes, making in this way the usage of High Performance Computing (HPC) technologies such as the message passing interface (MPI) a promising alternative. Recently, MetaCacheSpark, an Apache Spark based software for detection and quantification of species composition in food samples has been proposed. This tool can be used to analyze high throughput sequencing data sets of metagenomic DNA and allows for dealing with large-scale collections of complex eukaryotic and bacterial reference genome. In this work, we propose MetaCache-MPI, a fast and memory efficient solution for computing clusters which is based on MPI instead of Apache Spark. In order to evaluate its performance a comparison is performed between the original single CPU version of MetaCache, the Spark version and the MPI version we are introducing. Results show that for 32 processes, MetaCache-MPI is 1.65× faster while consuming 48.12\% of the RAM memory used by Spark for building a metagenomics database. For querying this database, also with 32 processes, the MPI version is 3.11× faster, while using 55.56\% of the memory used by Spark. We conclude that the new MetaCache-MPI version is faster in both building and querying the database and uses less RAM memory, when compared with MetaCacheSpark, while keeping the accuracy of the original implementation.},
    number = {10},

}

@misc{cpp_ref_vector,
	title={cppreference.com - Vector},
	howpublished={\url{https://en.cppreference.com/w/cpp/container/vector}} 
}

@article{use,
author = {Seffah, Ahmed and Donyaee, Mohammad and Kline, Rex and Padda, Harkirat},
year = {2006},
month = {06},
pages = {159-178},
title = {Usability measurement and metrics: A consolidated model},
volume = {14},
journal = {Software Quality Journal},
doi = {10.1007/s11219-006-7600-8}
}
@book{Sipser_2006, place={Boston}, edition={2nd ed}, title={Introduction to the theory of computation}, ISBN={9780534950972}, publisher={Thomson Course Technology}, author={Sipser, Michael}, year={2006} }

@book{Greenlaw_Hoover_Ruzzo_1995, place={New York}, title={Limits to parallel computation: P-completeness theory}, ISBN={9780195085914}, publisher={Oxford University Press}, author={Greenlaw, Raymond and Hoover, H. James and Ruzzo, Walter L.}, year={1995} }

@article{Alvarez_Greenlaw_2000, title={A compendium of problems complete for symmetric logarithmic space:}, volume={9}, ISSN={1016-3328}, DOI={10.1007/PL00001603}, number={2}, journal={Computational Complexity}, author={Alvarez, C. and Greenlaw, R.}, year={2000}, month={Dec}, pages={123–145} }

@book{Stallings_2010, place={Upper Saddle River, NJ}, edition={8th ed}, title={Computer organization and architecture: designing for performance}, ISBN={9780136073734}, publisher={Prentice Hall}, author={Stallings, William}, year={2010} }

@article{Chou_DeWitt_1986, title={An evaluation of buffer management strategies for relational database systems}, volume={1}, ISSN={0178-4617, 1432-0541}, DOI={10.1007/BF01840450}, number={1–4}, journal={Algorithmica}, author={Chou, Hong -Tai and DeWitt, David J.}, year={1986}, month={Nov}, pages={311–336} }
