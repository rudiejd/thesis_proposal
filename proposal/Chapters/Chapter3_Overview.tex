\chapter{Overview and Terminology} \label{overview_term}
As of right now, PC2L will be a \textbf{header-only} library. In order to initialize PC2L for a project, one must use the singleton \texttt{System} class, which contains system-wide settings, information, and shared objects. Settings on this class will determine the number of nodes that should be used in a run of PC2L. After this class is successfully obtained, the user can start PC2L, use data structures and algorithms, and then close PC2L when its features are no longer necessary, terminating all distributed processes. In this way, PC2L can be easily integrated into any portion of an existing application, allowing users to trivially transition from serial execution on one machine to distributed computing, then back to serial computing again if necessary. In any given instance of PC2L, there can be an arbitrary number of distributed \texttt{Worker} objects which handle the message passing work that would typically be done by users in a traditional distributed computing workflow. PC2L also includes a mode in which only one node in a cluster can write data, while the rest of the nodes are used as a distributed cache. This mode of operation allows for maximal utilization of cluster-wide memory and eliminates the potential for write-collisions, making it useful for applications in which a large in-memory data structure is constructed then queried many times. 

\section{Message Passing}
The main goal of this thesis is to deliver a portable library for parallel/distributed computing that is optimized for working with data intensive applications. In order to achieve this, we need some form of communication across nodes in a supercomputing cluster and/or processes on a local machine. Instead of reinventing the wheel by developing our own message passing protocol, we will stick with the industry standard Message Passing Interface (MPI) \cite{mpi}. MPI was standardized by 60 people from 40 top computing organizations across the United States including researchers, computer manufacturers, laboratories, and major companies. Nine versions of MPI have come out since the protocol's debut in 1994, with varying degrees of acceptance and adoption from the supercomputing community, but its core functionality is used almost universally in all high performance computing projects. Despite MPI's status as the \textit{de facto} standard for message passing, other alternatives to MPI that have gained more popularity in industry. Among these are Spark \cite{spark}, Charm++ \cite{parallel_programming_w_charm}, and ARMCI \cite{ARMCI}. However, Charm++ is actually an abstraction over MPI, utilizing it is a hidden backend, ARMCI does not deliver nearly all of the same features as the MPI standard, and, as demonstrated by one recent study of a big memory application in metagenomics \cite{metagenomics}, MPI applications are typically much faster and much less memory hungry than their Spark equivalents. 

In PC2L, the main purposes for message passing will be memory synchronization among nodes and instruction delivery to distributed workers. For instance, a \texttt{pc2l::vector} may be stored across several nodes, with distinct or overlapping sub-vectors in each node. If a program is operating on only one section of the vector corresponding to only one node, these changes can be stored in a node-level cache initially, then any caches storing the same data, along with workers operating on this data, can be informed of changes via an update message.  

\subsection{Message Class}
Instead of directly interfacing with an MPI implementation through the typical MPI directives, PC2L utilizes a \texttt{Message} class for internal consumption that abstracts most of these features away. This level of abstraction allows the underlying MPI commands to be changed without modifying the code in every single file across PC2L. Additionally, it opens the door for version of PC2L that are compatible with different versions of MPI, perhaps utilizing some more recently features in different versions. All of the MPI commands utilized in PC2L are redefined as macros within an internal MPI-helper header. 

\section{Workers}
Classes that inherit the \texttt{Worker} interface are used to abstract away different kinds of message passing in PC2L. At a minimum, any class that implements the \texttt{Worker} interface runs on a process with non-zerio MPI rank, sends messages to other \texttt{Worker} classes, and contains a buffer used to keep track of received messages. Workers will initially only be concerned with storing and retrieving information from a PC2L instance's distributed Cache, but different \texttt{Worker} variations will likely be added as the project progresses and different needs arise. 

\subsection{CacheWorker} 
A \texttt{CacheWorker} in PC2L is responsible for sending and receving cache-blocks to and from different processes. In order to best utilize the RAM in each separate node, each \texttt{CacheWorker} process will be run on a different compute node. When a \texttt{CacheWorker} is sent a message to store or retrieve a block of data, it will compute a composite key that combines that data structure type within which the block is stored with a unique block tag generated at run-time, allowing for $\mathcal{O}(1)$ address retrieval for members within a given distributed container. 
\subsection{CacheManager}
While there is one \texttt{CacheWorker} for every node included in a run of PC2L, there is only one \texttt{CacheManager} overall system-wide. The \texttt{CacheManager} for a given run of PC2l will run on the process with MPI rank zero, and will coordinate each \texttt{CacheWorker} across the system. Data structures in the program will communicate with the \texttt{CacheManger}, which will then schedule tasks to be performed on one of the many \texttt{CacheWorkers} using message passing.  


